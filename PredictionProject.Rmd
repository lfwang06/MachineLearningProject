---
title: "Predict How Well an Activity was Performed"
output:
  html_document:
    keep_md: yes
---

## Synopsis
Using devices such as Jawbone UP, Nike FuelBand and Fitbit is now possible to collection a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantity how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 young health participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Our goal is to build a machine learning algorithm to predict activity quality.

## Getting and processing data
The Weight Lifting Exercises Data for this project come from [this source] (http://groupware.les.inf.puc-rio.br/har). We download the training data [pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and the test data [pml-tesing.csv] (https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) , and use RStudio to read these two raw csv files. The original training data WleTrain have 19622 observations and 160 variables.
```{r,echo=TRUE}
wleTrain<-read.csv("pml-training.csv")
wleTest<-read.csv("pml-testing.csv")
dim(wleTrain)
dim(wleTest)
```
We find that many columns contain missing values and summarizing information such as maximum values and average values. These variables are not supposed to be good predictors, so we subset the wleTrain data and wleTest data to get rid of them. The resulting subsets have 59 variables.
```{r,echo=TRUE}
wleTrain<-subset(wleTrain, select=c(user_name:total_accel_belt,gyros_belt_x:total_accel_arm,gyros_arm_x:magnet_arm_z,roll_dumbbell:yaw_dumbbell,total_accel_dumbbell,gyros_dumbbell_x:yaw_forearm,total_accel_forearm,gyros_forearm_x:classe))
wleTest<-subset(wleTest, select=c(user_name:total_accel_belt,gyros_belt_x:total_accel_arm,gyros_arm_x:magnet_arm_z,roll_dumbbell:yaw_dumbbell,total_accel_dumbbell,gyros_dumbbell_x:yaw_forearm,total_accel_forearm,gyros_forearm_x:problem_id))
wleTest<-transform(wleTest,magnet_dumbbell_z=as.numeric(magnet_dumbbell_z), magnet_forearm_y=as.numeric(magnet_forearm_y),magnet_forearm_z=as.numeric(magnet_forearm_z))
dim(wleTrain)
dim(wleTest)
```

## Fitting models
Out goal is to fit a model with the variable classe as the outcome. Classe is a factor variable with 5 levels of A,B,C,D, and E in which A means exactly according to the specification, B means throwing the elbows to the front, C means lifting the dumbbell only halfway, D means lowering the dumbbell only halfway, E means throwing the hips to the front. The regression models are not appropriate for multi-class categorical outcomes. We will use tree-based methods to predict the classe variable to find out how well a human activity was performed.

First we want to create another testing set to estimate the out of sample error of the fitted model. We run a 70% 30% Split on the wleTrain data to get the training data and testing data.
```{r,echo=TRUE}
library(caret)
set.seed(2008)
inTrain = createDataPartition(wleTrain$classe, p = .7,list=FALSE)
training = wleTrain[ inTrain,]
testing = wleTrain[-inTrain,]
dim(training)
```
The image below shows some patterns between the outcome and predictors.
```{r,echo=TRUE}
library(ggplot2)
qplot(raw_timestamp_part_2, num_window, col=classe, data=training)
```

We use Principal Components Analysis (PCA) to reduce the number of predictors and then fit a random forest model using PCA with principal components explaining 95% of the variance in the predictors.
```{r,echo=TRUE}
# Do PCA on numerical variables.
pre<-preProcess(training[,-c(1,4,5,59)],method="pca",thresh=0.95)
pre
trainPC<-predict(pre,training[,-c(1,4,5,59)])
trainPC2<-cbind(trainPC,training[,c(1,4,5,59)])
library(randomForest)
set.seed(2008)
pcamodel<-randomForest(classe~.,data=trainPC2,ntree=300)
pcamodel
```
The estimate of error rate is 1.62%. The mean this model with CPA is good.

We fit another random forest model using k-fold cross validation relating classe to the remaining variables as they are. 
```{r,echo=TRUE}
set.seed(2008)
trainmodel<-train(classe~.,data=training,method="rf",trControl=trainControl(method="cv",number=5))
trainmodel
```
The model with 5-fold cross validation has amazing accuracy rate nearly to 1. The estimate error rate is very close to 0. So PCA doesn't improve the model accuracy. We choose the model without PCA and test it on the testing data.
```{r,echo=TRUE}
pred<-predict(trainmodel,testing)
confusionMatrix(pred,testing$classe)
```
The out of sample error of this model is 0.1% which is quite small. The model with 0.999 accuracy is a great fit.

## Predicting 20 different test cases.
```{r,echo=TRUE}
wleTest$classe<-factor(c("A","B","C","D","E"))
pred20<-predict(trainmodel, wleTest[,-59])
pred20
```
